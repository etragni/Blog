---
layout: post
autore: "Evangelista Tragni"
title: Install Tanzu User Managed Packages 
comments: false
category: blog
---

# Tanzu User Managed Packages Prerequisites

## Install kubectl and kubectl-vsphere

Navigate to the `Supervisor IP Address` .

Download the Kubectl-vsphere CLI for your OS.

For this Example I will download the  `CLI PLUGIN MAC OS` 



## Install Tanzu CLI

To download and unpack Tanzu CLI:

Go to https://my.vmware.com and log in with your My VMware credentials.

Visit the [Tanzu Kubernetes Grid downloads page](https://customerconnect.vmware.com/en/downloads/info/slug/infrastructure_operations_management/vmware_tanzu_kubernetes_grid/1_x)

In the VMware Tanzu Kubernetes Grid row, click `Go to Downloads`.

In the Select Version drop-down, select `1.4.2`.

Under Product Downloads, scroll to the section labeled VMware Tanzu CLI 1.4.2 CLI:
For This example , I will download the `VMware Tanzu CLI for Mac` and click Download Now.

In the Download folder, unpack the Tanzu CLI bundle file for your operating system. To unpack the bundle file, use the extraction tool of your choice. For example, the `tar -xvf` command.
```
tar -xvf tanzu-cli-bundle-darwin-amd64.tar
```
Navigate to the cli subfolder under the tanzu folder that you unpacked in the previous section:
```
❯ cd cli/
```
Confirm that the binary is executable by running the ls command.
```
❯ ls
```

> ```
> cluster                                 management-cluster
> core                                    manifest.yaml
> imgpkg-darwin-amd64-v0.10.0+vmware.1.gz package
> kapp-darwin-amd64-v0.37.0+vmware.1.gz   pinniped-auth
> kbld-darwin-amd64-v0.30.0+vmware.1.gz   vendir-darwin-amd64-v0.21.1+vmware.1.gz
> kubernetes-release                      ytt-darwin-amd64-v0.34.0+vmware.1.gz
> login
> ```

Install the binary to /usr/local/bin:

```
❯ sudo install core/v1.4.2/tanzu-core-darwin_amd64 /usr/local/bin/tanzu
```

At the command line in a new terminal, run tanzu version to check that the correct version of the CLI is properly installed:
```
❯ tanzu version
```

> ```
> version: v1.4.2
> buildDate: 2022-02-01
> sha: 0732f1da
> ```

## Install the Tanzu CLI Plugins

After you have installed the tanzu core executable, you must install the CLI plugins related to Tanzu Kubernetes cluster management and feature operations.

Navigate to the tanzu folder that contains the cli folder.
```
cd Downloads
```

Run the following command from the tanzu directory to install all the plugins for this release.
```
tanzu plugin install --local cli all
```

Check plugin installation status.
```
tanzu plugin list
```
If successful, you should see a list of all installed plugins. For example:

> ```
>   NAME                LATEST VERSION  DESCRIPTION                                                        REPOSITORY  VERSION  STATUS
>   alpha               v1.4.1          Alpha CLI commands                                                 core                 not installed
>   cluster             v1.4.1          Kubernetes cluster operations                                      core        v1.4.2   installed
>   kubernetes-release  v1.4.1          Kubernetes release operations                                      core        v1.4.2   installed
>   login               v1.4.1          Login to the platform                                              core        v1.4.2   installed
>   management-cluster  v1.4.1          Kubernetes management cluster operations                           core        v1.4.2   installed
>   package             v1.4.1          Tanzu package management                                           core        v1.4.2   installed
>   pinniped-auth       v1.4.1          Pinniped authentication operations (usually not directly invoked)  core        v1.4.2   installed
> ```

## Install the Carvel Tools

Carvel provides a set of reliable, single-purpose, composable tools that aid in application building, configuration, and deployment to Kubernetes.

Tanzu Kubernetes Grid uses the following tools from the Carvel open-source project:

ytt - a command-line tool for templating and patching YAML files. You can also use ytt to collect fragments and piles of YAML into modular chunks for easy re-use.

kapp - the applications deployment CLI for Kubernetes. It allows you to install, upgrade, and delete multiple Kubernetes resources as one application.

kbld - an image-building and resolution tool.

imgpkg - a tool that enables Kubernetes to store configurations and the associated container images as OCI images, and to transfer these images.

### Install Ytt

Open the cli folder:
```
cd Downloads/cli
```
Unpack the `ytt` binary and make it executable.
```
gunzip ytt-darwin-amd64-v0.34.0+vmware.1.gz
chmod ugo+x ytt-darwin-amd64-v0.34.0+vmware.1
```
Move the binary to `/usr/local/bin` and rename it to ytt:
```
mv ./ytt-darwin-amd64-v0.34.0+vmware.1 /usr/local/bin/ytt
```
At the command line in a new terminal, run `ytt version` to check that the correct version of ytt is properly installed.
```
❯ ytt version
```
> ```
> ytt version 0.34.0
> ```

### Install Kapp

Unpack the `kapp` binary and make it executable.
```
gunzip kapp-darwin-amd64-v0.37.0+vmware.1.gz
chmod ugo+x kapp-darwin-amd64-v0.37.0+vmware.1
```

Move the binary to `/usr/local/bin` and rename it to kapp:
```
mv ./kapp-darwin-amd64-v0.37.0+vmware.1 /usr/local/bin/kapp
```
At the command line in a new terminal, run `kapp version` to check that the correct version of kapp is properly installed.
```
kapp version
```
> ```
> kapp version 0.37.0  
> Succeeded
> ```

### Install Kbld

Unpack the `kbld` binary and make it executable.
```
gunzip kbld-darwin-amd64-v0.30.0+vmware.1.gz
chmod ugo+x kbld-darwin-amd64-v0.30.0+vmware.1
```
Move the binary to `/usr/local/bin` and rename it to kbld:
```
mv ./kbld-darwin-amd64-v0.30.0+vmware.1 /usr/local/bin/kbld
```
At the command line in a new terminal, run `kbld version` to check that the correct version of kbld is properly installed.
```
kbld version 
```
> ```
> kbld version 0.30.0
> 
> Succeeded
> ```

### Install Imgpkg

Unpack the imgpkg binary and make it executable.
```
gunzip imgpkg-darwin-amd64-v0.10.0+vmware.1.gz
chmod ugo+x imgpkg-darwin-amd64-v0.10.0+vmware.1
```
Move the binary to /usr/local/bin and rename it to imgpkg:
```
mv ./imgpkg-darwin-amd64-v0.10.0+vmware.1 /usr/local/bin/imgpkg
```
At the command line in a new terminal, run imgpkg version to check that the correct version of imgpkg is properly installed.
```
imgpkg version
```
> ```
> imgpkg version 0.10.0
> 
> Succeeded
> ```

# User Managed Packages

A package is a collection of related software that supports or extends the core functionality of the Kubernetes cluster in which the package is installed.

- `Core packages` are automatically installed and managed by Tanzu Kubernetes Grid. These packages are located in the tanzu-core package repository.
- `User-managed` packages are installed and managed by you. These packages are located in the tanzu-standard package repository.

## Prepare Environment
If you haven’t already, add the workload cluster created by using the Tanzu Kubernetes Grid service’s Supervisor Cluster to the Tanzu CLI by running commands similar to below.

Login to the `vSphere` environment:

```
kubectl vsphere login --insecure-skip-tls-verify --server <SUPERVISOR_IP> --vsphere-username <VSPHERE_USERNAME> --tanzu-kubernetes-cluster-namespace <NAMESPACE> --tanzu-kubernetes-cluster-name <TKGS_CLUSTER>
```
In my example it is something like this:

> ```
> kubectl vsphere login --insecure-skip-tls-verify --server 10.10.10.10 --vsphere-username e.tragni@desotech.local --tanzu-kubernetes-cluster-namespace hpe-test --tanzu-kubernetes-cluster-name tkgs-cluster-packages
> ```

Login on te `Tanzu CLI`:
```
kubectl config use-context <TKGS_CLUSTER> 
tanzu login --name <choose-a-name-for-supervisor-cluster> --kubeconfig ~/.kube/config --context SUPERVISOR-IP
```
In my environment it is like this:

> ```
> kubectl config use-context tkgs-cluster-packages
> tanzu login --name my-super --kubeconfig ~/.kube/config  --context 10.10.1.2
> ```

> ```
> ✔  successfully logged in to management cluster using the kubeconfig my-super
> ```
> 

```
tanzu login
```
> ```
> ? Select a server  [Use arrows to move, type to filter]
> my-super            ()
>   + new server
> 
> ```


Check if a default storage class is defined:
```
kubectl get storageclass
```
> ```
> NAME                   PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
> tanzu-fast             csi.vsphere.vmware.com   Delete          Immediate           true                   4d18h
> tanzu-slow (default)   csi.vsphere.vmware.com   Delete          Immediate           true                   4d18h
> ```
If no default storage class is listed, specify one.

Create a file `tanzu-system-kapp-ctrl-restricted.yaml` containing the Kapp Controller Pod Security Policy below.
```
vi tanzu-system-kapp-ctrl-restricted.yaml
```
<!--codeinclude-->
[tanzu-system-kapp-ctrl-restricted](posts/kapp-controller-policy.yaml)
<!--/codeinclude-->
```
kubectl apply -f tanzu-system-kapp-ctrl-restricted.yaml
```

```
vi kapp-controller.yaml
```
[kapp-controller.yaml](kapp-controller.yaml)




```
tanzu package repository add repo-standard --url projects.registry.vmware.com/tkg/packages/standard/repo:v1.4.0
```
>```
> - Adding package repository 'repo-standard'...
>  Added package repository 'repo-standard'
>```



```
tanzu package repository list
```

>```
> - Retrieving repositories...
>   NAME           REPOSITORY                                                      STATUS               DETAILS
>   repo-standard  projects.registry.vmware.com/tkg/packages/standard/repo:v1.4.0  Reconcile succeeded
>```


```
tanzu package available list
```
>```
> \ Retrieving available packages...
>   NAME                           DISPLAY-NAME  SHORT-DESCRIPTION
>   cert-manager.tanzu.vmware.com  cert-manager  Certificate management
>   contour.tanzu.vmware.com       Contour       An ingress controller
>   external-dns.tanzu.vmware.com  external-dns  This package provides DNS synchronization functionality.
>   fluent-bit.tanzu.vmware.com    fluent-bit    Fluent Bit is a fast Log Processor and Forwarder
>   grafana.tanzu.vmware.com       grafana       Visualization and analytics software
>   harbor.tanzu.vmware.com        Harbor        OCI Registry
>   multus-cni.tanzu.vmware.com    multus-cni    This package provides the ability for enabling attaching multiple network interfaces to pods in Kubernetes
>   prometheus.tanzu.vmware.com    prometheus    A time series database for your metrics
>```


## Deploy Cert-Manager on Tanzu Cluster 
```
tanzu package available list cert-manager.tanzu.vmware.com -A
```

>```
> - Retrieving package versions for cert-manager.tanzu.vmware.com...
>   NAME                           VERSION               RELEASED-AT                    NAMESPACE
>   cert-manager.tanzu.vmware.com  1.1.0+vmware.1-tkg.2  2020-11-24 19:00:00 +0100 CET  default
>```


```
tanzu package install cert-manager --package-name cert-manager.tanzu.vmware.com --namespace default --version 1.1.0+vmware.1-tkg.2
```

>```
> - Installing package 'cert-manager.tanzu.vmware.com'
> | Getting namespace 'default'
> / Getting package metadata for 'cert-manager.tanzu.vmware.com'
> | Creating service account 'cert-manager-default-sa'
> | Creating cluster admin role 'cert-manager-default-cluster-role'
> | Creating cluster role binding 'cert-manager-default-cluster-rolebinding'
> - Creating package resource
> - Package install status: Reconciling
> 
> 
>  Added installed package 'cert-manager' in namespace 'default'
>```


```
tanzu package installed list -A
```
>```
> - Retrieving installed packages...
>   NAME          PACKAGE-NAME                   PACKAGE-VERSION       STATUS
>   cert-manager  cert-manager.tanzu.vmware.com  1.1.0+vmware.1-tkg.2  Reconcile succeeded
>```



---
## Deploy Contour on Tanzu Cluster
```
vi contour-data-values.yaml
```

[contour-data-values.yaml](contour-data-values.yaml)

```
kubectl create clusterrolebinding envoy-tkg-admin-privileged-binding --clusterrole=psp:vmware-system-privileged --serviceaccount=tanzu-system-ingress:envoy
```

>```
> clusterrolebinding.rbac.authorization.k8s.io/envoy-tkg-admin-privileged-binding created
>```


```
tanzu package available list contour.tanzu.vmware.com -A
```

>```
> - Retrieving package versions for contour.tanzu.vmware.com...
>   NAME                      VERSION                RELEASED-AT                     NAMESPACE
>   contour.tanzu.vmware.com  1.17.1+vmware.1-tkg.1  2021-07-23 20:00:00 +0200 CEST  default
>```


```
tanzu package install contour \
--package-name contour.tanzu.vmware.com \
--version 1.17.1+vmware.1-tkg.1 \
--values-file contour-data-values.yaml \
--namespace default
```

>```
> - Installing package 'contour.tanzu.vmware.com'
> | Getting namespace 'default'
> / Getting package metadata for 'contour.tanzu.vmware.com'
> | Creating service account 'contour-default-sa'
> | Creating cluster admin role 'contour-default-cluster-role'
> | Creating cluster role binding 'contour-default-cluster-rolebinding'
> | Creating secret 'contour-default-values'
> - Creating package resource
> / Package install status: Reconciling
> 
> 
>  Added installed package 'contour' in namespace 'default'
>```


```
tanzu package installed list -A
```

>```
> \ Retrieving installed packages...
>   NAME          PACKAGE-NAME                   PACKAGE-VERSION        STATUS               NAMESPACE
>   cert-manager  cert-manager.tanzu.vmware.com  1.1.0+vmware.1-tkg.2   Reconcile succeeded  default
>   contour       contour.tanzu.vmware.com       1.17.1+vmware.1-tkg.1  Reconcile succeeded  default
>```

```
kubectl get apps -A
```

>```
> NAMESPACE   NAME           DESCRIPTION           SINCE-DEPLOY   AGE
> default     cert-manager   Reconcile succeeded   35s            18m
> default     contour        Reconcile succeeded   56s            5m42s
>```

```
kubectl get pods -A
```

>```
> ...
> tanzu-system-ingress           contour-56bbb95977-ch946                                                          1/1     Running   0          6m27s
> tanzu-system-ingress           contour-56bbb95977-gm7xd                                                          1/1     Running   0          6m27s
> tanzu-system-ingress           envoy-669ct                                                                       2/2     Running   0          6m28s
> tanzu-system-ingress           envoy-jhjs5                                                                       2/2     Running   0          6m28s
> tanzu-system-ingress           envoy-nm8hw                                                                       2/2     Running   0          6m28s
> ...
>```

```
ENVOY_POD=$(kubectl -n tanzu-system-ingress get pod -l app=envoy -o name | head -1)
```


```
kubectl -n tanzu-system-ingress port-forward $ENVOY_POD 9001
```

>```
> Forwarding from 127.0.0.1:9001 -> 9001
> Forwarding from [::1]:9001 -> 9001
> 
>```


navigate to http://127.0.0.1:9001/

You should see the Envoy administration interface.

![contour](/Images/tanzu-packages/contour-1.png)


### Visualize the Internal Contour Directed Acyclic Graph (DAG)

When you have started running workloads in your cluster, you can visualize the traffic information that Contour exposes in the form of a directed acyclic graph (DAG).

Obtain the name of a Contour pod:

```
CONTOUR_POD=$(kubectl -n tanzu-system-ingress get pod -l app=contour -o name | head -1)
```

Forward port 6060 on the Contour pod:
```
kubectl -n tanzu-system-ingress port-forward $CONTOUR_POD 6060
```
>```
> Forwarding from 127.0.0.1:6060 -> 6060
> Forwarding from [::1]:6060 -> 6060
>```

Open a new terminal window and download and save the DAG as a *.png file. The below command requires you to install dot on your system if it is not present already.

> Note :
>  Install dot 
>  Brew install graphviz

```
curl localhost:6060/debug/dag | dot -T png > contour-dag.png
```
>```
>   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
>                                  Dload  Upload   Total   Spent    Left  Speed
> 100    29  100    29    0     0    273      0 --:--:-- --:--:-- --:--:--   292
>```

Open contour-dag.png to view the graph.


## Deploy Fluent Bit on a Tanzu Kubernetes Cluster

```
tanzu package available list fluent-bit.tanzu.vmware.com -A
```

>```
> | Retrieving package versions for fluent-bit.tanzu.vmware.com...
>   NAME                         VERSION               RELEASED-AT                     NAMESPACE
>   fluent-bit.tanzu.vmware.com  1.7.5+vmware.1-tkg.1  2021-05-13 20:00:00 +0200 CEST  default
>```

Retrieve the template from the package itself by following the procedure described in Retrieve the Data Values Template.
```
image_url=$(kubectl -n default get packages fluent-bit.tanzu.vmware.com.1.7.5+vmware.1-tkg.1 -o jsonpath='{.spec.template.spec.fetch[0].imgpkgBundle.image}')
```

```
echo $image_url
```
>```
> projects.registry.vmware.com/tkg/packages/standard/fluent-bit@sha256:c83d038c57f244aae2819dd77dc5184bb3e1ec96524d3f6a09fe8a244b7bc9e4
>```

Run imgpkg with image_url to retrieve the package image bundle:

```
imgpkg pull -b $image_url -o PACKAGE-DIR
```

Where `PACKAGE-DIR` is a local directory to save the bundle to. For example:

>```
> Pulling bundle 'projects.registry.vmware.com/tkg/packages/standard/fluent-bit@sha256:c83d038c57f244aae2819dd77dc5184bb3e1ec96524d3f6a09fe8a244b7bc9e4'
> Extracting layer 'sha256:48456e4452a35786ed148a3f1a9ede1427f8fccb0079a0eb0904820211b6cebd' (1/1)
> 
> Locating image lock file images...
> The bundle repo (projects.registry.vmware.com/tkg/packages/standard/fluent-bit) is hosting every image specified in the bundle's Images Lock file (.imgpkg/images.yml)
> 
> Succeeded
>```

give the file a package-specific name and move it to the current directory

```sh
cp PACKAGE-DIR/config/values.yaml fluent-bit-data-values.yaml
```


After you make any changes needed to your fluent-bit-data-values.yaml file, remove all comments in it:
```
yq -i eval '... comments=""' fluent-bit-data-values.yaml
```

```
tanzu package install fluent-bit \
--package-name fluent-bit.tanzu.vmware.com \
--version 1.7.5+vmware.1-tkg.1 \
--values-file fluent-bit-data-values.yaml \
--namespace default
```
>```
> \ Installing package 'fluent-bit.tanzu.vmware.com'
> | Getting namespace 'default'
> / Getting package metadata for 'fluent-bit.tanzu.vmware.com'
> | Creating service account 'fluent-bit-default-sa'
> | Creating cluster admin role 'fluent-bit-default-cluster-role'
> | Creating cluster role binding 'fluent-bit-default-cluster-rolebinding'
> | Creating secret 'fluent-bit-default-values'
> - Creating package resource
> - Package install status: Reconciling
> 
> 
>  Added installed package 'fluent-bit' in namespace 'default'
>```

```
tanzu package installed list -A
```

>```
> \ Retrieving installed packages...
>   NAME          PACKAGE-NAME                   PACKAGE-VERSION        STATUS               NAMESPACE
>   cert-manager  cert-manager.tanzu.vmware.com  1.1.0+vmware.1-tkg.2   Reconcile succeeded  default
>   contour       contour.tanzu.vmware.com       1.17.1+vmware.1-tkg.1  Reconcile succeeded  default
>   fluent-bit    fluent-bit.tanzu.vmware.com    1.7.5+vmware.1-tkg.1   Reconcile succeeded  default
>```


```
kubectl get apps -A
```
>```
> NAMESPACE   NAME           DESCRIPTION           SINCE-DEPLOY   AGE
> default     cert-manager   Reconcile succeeded   55s            16h
> default     contour        Reconcile succeeded   36s            16h
> default     fluent-bit     Reconcile succeeded   18s            3m33s
>```


## Deploy Prometheus into the Workload Cluster
```
tanzu package available list prometheus.tanzu.vmware.com -A
```
>```
> \ Retrieving package versions for prometheus.tanzu.vmware.com...
>   NAME                         VERSION                RELEASED-AT                     NAMESPACE
>   prometheus.tanzu.vmware.com  2.27.0+vmware.1-tkg.1  2021-05-12 20:00:00 +0200 CEST  default
>```

### Deploy Prometheus with Default Configurations

After you confirm the package version and retrieve it, you can install the package.

Install the Prometheus package using its default values:


```
tanzu package install prometheus \
--package-name prometheus.tanzu.vmware.com \
--version PACKAGE-VERSION \
--namespace TARGET-NAMESPACE
```
Where:

`TARGET-NAMESPACE` is the namespace in which you want to install the Prometheus package and deploy the Prometheus package app, which is managed by kapp-controller. For example, default. If this flag is not specified, the Tanzu CLI uses the default namespace. The Prometheus pods and any other resources associated with the Prometheus component are created in the tanzu-system-metrics namespace; do not install the Prometheus package into this namespace.
`PACKAGE-VERSION` is the version that you retrieved above, for example 2.27.0+vmware.1-tkg.1.
For example:

```
tanzu package install prometheus \
--package-name prometheus.tanzu.vmware.com \
--version 2.27.0+vmware.1-tkg.1 \
--namespace default
```


>```
> \ Installing package 'prometheus.tanzu.vmware.com'
> | Creating namespace 'default'
> / Getting package metadata for 'prometheus.tanzu.vmware.com'
> | Creating service account 'prometheus-default-sa'
> | Creating cluster admin role 'prometheus-default-cluster-role'
> | Creating cluster role binding 'prometheus-default-cluster-rolebinding'
> - Creating package resource
> \ Package install status: Reconciling
> 
> Added installed package 'prometheus' in namespace 'default'
>```

### Deploy Prometheus with Custom Values

To install the Prometheus package using user-provided values:

Retrieve the template of the Prometheus package’s default configuration:
```
image_url=$(kubectl -n default get packages prometheus.tanzu.vmware.com.2.27.0+vmware.1-tkg.1 -o jsonpath='{.spec.template.spec.fetch[0].imgpkgBundle.image}')
```
```
imgpkg pull -b $image_url -o /tmp/prometheus-package-2.27.0+vmware.1-tkg.1
```
>```
> Pulling bundle 'projects.registry.vmware.com/tkg/packages/standard/prometheus@sha256:27af034c1c77bcae4e1f7f6d3883286e34419ea2e88222642af17393cd34e46a'
>   Extracting layer 'sha256:44798ebd112b55ea792f0198cf220a7eaed37c2abc531d6cf8efe89aadc8bff2' (1/1)
> 
> Locating image lock file images...
> One or more images not found in bundle repo; skipping lock file update
> 
> Succeeded
>```


```
cp /tmp/prometheus-package-2.27.0+vmware.1-tkg.1/config/values.yaml prometheus-data-values.yaml
```

This creates a configuration file named prometheus-data-values.yaml that you can modify. See Retrieve the Data Values Template for more about this sequence of commands.

For information about configuration parameters to use in prometheus-data-values.yaml, see Prometheus Package Configuration Parameters below.

**vSphere with Tanzu**: If you are deploying Prometheus to a workload cluster created by using the Tanzu Kubernetes Grid service in vSphere 7.0 U2, set a non-null value for prometheus.pvc.storageClassName and alertmanager.pvc.storageClassName in the prometheus-data-values.yaml file:
```
vi prometheus-data-values.yaml
```

Change the following value:
**false to true** 

Where STORAGE-CLASS is the name of the cluster’s storage class, as returned by kubectl get storageclass.
>```yaml
> ingress:
>   enabled: `false`
>   virtual_host_fqdn: "prometheus.corp.tanzu"
>   prometheus_prefix: "/"
>   alertmanager_prefix: "/alertmanager/"
>   prometheusServicePort: 80
>   alertmanagerServicePort: 80
> prometheus:
>   pvc:
>     storageClassName: `STORAGE-CLASS`
> alertmanager:
>   pvc:
>     storageClassName: `STORAGE-CLASS`
>```



After you make any changes needed to your prometheus-data-values.yaml file, remove all comments in it:
```
yq -i eval '... comments=""' prometheus-data-values.yaml
```

Deploy the package:

```
tanzu package install prometheus \
--package-name prometheus.tanzu.vmware.com \
--version PACKAGE-VERSION \
--values-file prometheus-data-values.yaml \
--namespace TARGET-NAMESPACE
```
Where:

`TARGET-NAMESPACE` is the namespace in which you want to install the Prometheus package and deploy the Prometheus package app, which is managed by kapp-controller. For example, default. If this flag is not specified, the Tanzu CLI uses the default namespace. The Prometheus pods and any other resources associated with the Prometheus component are created in the tanzu-system-metrics namespace; do not install the Prometheus package into this namespace.
`PACKAGE-VERSION` is the version that you retrieved above, for example 2.27.0+vmware.1-tkg.1.


```
tanzu package install prometheus \
--package-name prometheus.tanzu.vmware.com \
--version 2.27.0+vmware.1-tkg.1 \
--values-file prometheus-data-values.yaml \
--namespace default
```
>```
> | Installing package 'prometheus.tanzu.vmware.com'
> | Getting namespace 'default'
> / Getting package metadata for 'prometheus.tanzu.vmware.com'
> | Creating service account 'prometheus-default-sa'
> | Creating cluster admin role 'prometheus-default-cluster-role'
> | Creating cluster role binding 'prometheus-default-cluster-rolebinding'
> / Creating secret 'prometheus-default-values'
> - Creating package resource
> \ Package install status: Reconciling
>```

```
tanzu package installed list -A
```
>```
> \ Retrieving installed packages...
>   NAME          PACKAGE-NAME                   PACKAGE-VERSION        STATUS               NAMESPACE
>   cert-manager  cert-manager.tanzu.vmware.com  1.1.0+vmware.1-tkg.2   Reconcile succeeded  default
>   contour       contour.tanzu.vmware.com       1.17.1+vmware.1-tkg.1  Reconcile succeeded  default
>   fluent-bit    fluent-bit.tanzu.vmware.com    1.7.5+vmware.1-tkg.1   Reconcile succeeded  default
>   prometheus    prometheus.tanzu.vmware.com    2.27.0+vmware.1-tkg.1  Reconciling          default
>```

### Note

**vSphere with Tanzu**

On vSphere 7.0 U2, the tanzu package install prometheus command may return the error Failed to get final advertise address: No private IP address found, and explicit IP not provided.


To fix this error, create and apply a package overlay to reconfigure the alertmanager component:

Create a file `overlay-alertmanager.yaml` containing:

```
---
#@ load("@ytt:overlay", "overlay")

#@overlay/match by=overlay.and_op(overlay.subset({"kind": "Deployment"}), overlay.subset({"metadata": {"name": "alertmanager"}}))
---
spec:
  template:
    spec:
      containers:
        #@overlay/match by="name",expects="0+"
        - name: alertmanager
          args:
            - --cluster.listen-address=
```

Create a secret from the overlay:
```
kubectl create secret generic alertmanager-overlay -n default -o yaml --dry-run=client --from-file=overlay-alertmanager.yaml | kubectl apply -f -
```

Annotate the package with the secret:

```
kubectl annotate PackageInstall prometheus -n default ext.packaging.carvel.dev/ytt-paths-from-secret-name.1=alertmanager-overlay
```


It will **automatically** reconcile the Prometheus package

```
tanzu package installed list -A
```
>```
> \ Retrieving installed packages...
>   NAME          PACKAGE-NAME                   PACKAGE-VERSION        STATUS               NAMESPACE
>   cert-manager  cert-manager.tanzu.vmware.com  1.1.0+vmware.1-tkg.2   Reconcile succeeded  default
>   contour       contour.tanzu.vmware.com       1.17.1+vmware.1-tkg.1  Reconcile succeeded  default
>   fluent-bit    fluent-bit.tanzu.vmware.com    1.7.5+vmware.1-tkg.1   Reconcile succeeded  default
>   prometheus    prometheus.tanzu.vmware.com    2.27.0+vmware.1-tkg.1  Reconcile succeeded  default
>```

```
kubectl get apps -A
```
>```
> NAMESPACE   NAME           DESCRIPTION           SINCE-DEPLOY   AGE
> default     cert-manager   Reconcile succeeded   40s            18h
> default     contour        Reconcile succeeded   33s            18h
> default     fluent-bit     Reconcile succeeded   28s            126m
> default     prometheus     Reconcile succeeded   26s            24m
>```


Create a DNS record to map `prometheus.system.tanzu` to the External IP address of the Envoy load balancer.

```
kubectl get svc -n tanzu-system-ingress
```
>```
> NAMESPACE                 NAME                            TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
> tanzu-system-ingress      contour                         ClusterIP      198.54.13.185    <none>        8001/TCP                     18h
> tanzu-system-ingress      envoy                           LoadBalancer   198.54.213.239   10.10.1.9     80:31187/TCP,443:31903/TCP   18h
>```

Access the Prometheus dashboard by navigating to https://prometheus.system.tanzu in a browser.

![prometheus](/Images/tanzu-packages/prometheus.png)


## Deploy Grafana on Tanzu Kubernetes Clusters

```
tanzu package available list grafana.tanzu.vmware.com -A
```

> ```
> \ Retrieving package versions for grafana.tanzu.vmware.com...
>   NAME                      VERSION               RELEASED-AT                     NAMESPACE
>   grafana.tanzu.vmware.com  7.5.7+vmware.1-tkg.1  2021-05-19 20:00:00 +0200 CEST  default
> ```

Retrieve the template of the Grafana package’s default configuration:

```
image_url=$(kubectl -n default get packages grafana.tanzu.vmware.com.7.5.7+vmware.1-tkg.1 -o jsonpath='{.spec.template.spec.fetch[0].imgpkgBundle.image}')
```

```
imgpkg pull -b $image_url -o /tmp/grafana-package-7.5.7+vmware.1-tkg.1
```

>```
> Pulling bundle 'projects.registry.vmware.com/tkg/packages/standard/grafana@sha256:561e6afe31bf627f89dbb92a7dfded25128037fc407efc7d0190f2c6bc6deec7'
>   Extracting layer 'sha256:a6deb6b8b1e485dce2e45bea45a6055d502b945dc0abd22c47e54b9763c0486b' (1/1)
> 
> Locating image lock file images...
> The bundle repo (projects.registry.vmware.com/tkg/packages/standard/grafana) is hosting every image specified in the bundle's Images Lock file (.imgpkg/images.yml)
> 
> Succeeded
>```

```
cp /tmp/grafana-package-7.5.7+vmware.1-tkg.1/config/values.yaml grafana-data-values.yaml
```


This creates a configuration file named grafana-data-values.yaml that you can modify. See Retrieve the Data Values Template for more about this sequence of commands.

For information about configuration parameters to use in grafana-data-values.yaml, see Grafana Package Configuration Parameters below.

Set the namespace for the Grafana deployment in grafana-data-values.yaml as shown below:

>```
> #! The namespace in which to deploy grafana.
> namespace: tanzu-system-dashboards
>```

Edit grafana-data-values.yaml and replace secret.admin_password with a Base64-encoded password. 

To generate a Base64 encoded password, run:

```
echo -n 'mypassword' | base64
```

mypassword results in the encoded password bXlwYXNzd29yZA==.


**vSphere with Tanzu**: If you are deploying Grafana to a workload cluster created by using the Tanzu Kubernetes Grid service in vSphere 7.0 U2, set a non-null value for ingress.pvc.storageClassName in the grafana-data-values.yaml file:

>```
> ingress:
> virtual_host_fqdn: "grafana.system.tanzu"
> pvc:
>   storageClassName: STORAGE-CLASS
>```

Where STORAGE-CLASS is the name of the cluster’s storage class, as returned by kubectl get storageclass.

(Optional) Modify the Grafana datasource configuration in grafana-data-values.yaml. Grafana is configured with Prometheus as a default data source. If you have customized the Prometheus deployment namespace and it is not deployed in the default namespace, tanzu-system-monitoring, you need to change the Grafana datasource configuration in grafana-data-values.yaml.

To change the datasource configuration, copy the section below into the position shown and modify url as needed.

>```
> #! The namespace in which to deploy grafana.
> namespace: tanzu-system-dashboards
> 
> grafana:
>       datasources:
>         - name: Prometheus
>           type: prometheus
>           url: prometheus-server.<change-to-prometheus-namespace>.svc.cluster.local
>           access: proxy
>           isDefault: true
>```


After you make any changes needed to your grafana-data-values.yaml file, remove all comments in it:

```
yq -i eval '... comments=""' grafana-data-values.yaml
```

Deploy the package:

If the target namespace exists in the cluster, run:

```
tanzu package install grafana \
--package-name grafana.tanzu.vmware.com \
--version 7.5.7+vmware.1-tkg.1 \
--values-file grafana-data-values.yaml \
--namespace default
```

>```
> tanzu package install grafana --package-name grafana.tanzu.vmware.com --version 7.5.7+vmware.1-tkg.1 --values-file grafana-data-values.yaml --namespace default
> \ Installing package 'grafana.tanzu.vmware.com'
> | Getting namespace 'default'
> / Getting package metadata for 'grafana.tanzu.vmware.com'
> | Creating service account 'grafana-default-sa'
> | Creating cluster admin role 'grafana-default-cluster-role'
> | Creating cluster role binding 'grafana-default-cluster-rolebinding'
> | Creating secret 'grafana-default-values'
> - Creating package resource
> \ Package install status: Reconciling
> 
>  Added installed package 'grafana' in namespace 'default'
>```



### Verify Grafana Deployment


After you deploy Grafana, you can verify that the deployment is successful:

Confirm that the grafana package is installed. For example:
```
tanzu package installed list -A
```

>```
> \ Retrieving installed packages...
>   NAME          PACKAGE-NAME                   PACKAGE-VERSION        STATUS               NAMESPACE
>   cert-manager  cert-manager.tanzu.vmware.com  1.1.0+vmware.1-tkg.2   Reconcile succeeded  default
>   contour       contour.tanzu.vmware.com       1.17.1+vmware.1-tkg.1  Reconcile succeeded  default
>   fluent-bit    fluent-bit.tanzu.vmware.com    1.7.5+vmware.1-tkg.1   Reconcile succeeded  default
>   grafana       grafana.tanzu.vmware.com       7.5.7+vmware.1-tkg.1   Reconcile succeeded  default
>   prometheus    prometheus.tanzu.vmware.com    2.27.0+vmware.1-tkg.1  Reconcile succeeded  default
>```

The grafana package and its resources, such as the grafana app, are installed in the namespace that you specify when running the tanzu package install command.

Confirm that the grafana app is successfully reconciled:

```
kubectl get apps -A
```
>```
> NAMESPACE   NAME           DESCRIPTION           SINCE-DEPLOY   AGE
> default     cert-manager   Reconcile succeeded   50s            19h
> default     contour        Reconcile succeeded   55s            18h
> default     fluent-bit     Reconcile succeeded   21s            162m
> default     grafana        Reconcile succeeded   28s            6m8s
> default     prometheus     Reconcile succeeded   30s            60m
>```

If the status is not Reconcile succeeded, view the full status details of the grafana app. Viewing the full status can help you troubleshoot the problem:

Confirm that the new services are running by listing all of the pods that are running in the cluster:
```
kubectl get pods -A
```

In the tanzu-system-dashboards namespace, you should see the grafana service running in a pod:

>```
> NAMESPACE               NAME                                    READY   STATUS    RESTARTS   AGE
> ...
> tanzu-system-dashboards        grafana-59d77bb5d7-pv2g5         2/2     Running   0          6m38s
> ...
>```

The Grafana pods and any other resources associated with the Grafana component are created in the namespace you provided in grafana-data-values.yaml.

After Grafana is deployed, the grafana package creates a Contour HTTPProxy object with a Fully Qualified Domain Name (FQDN) of `grafana.system.tanzu`.

Create an entry in your local /etc/hosts file that points an IP address to this FQDN

To use this FQDN to access the Grafana dashboard:

```
Navigate to https://grafana.system.tanzu.
```

Logn with :
```
Username : admin
Password : mypassword
```

![graphana](/Images/tanzu-packages/graphana.png)


## Deploy Harbor Registry as a Shared Service


You can use the Harbor shared service as a private registry for images that you want to make available to all of the workload clusters that you deploy from a given management cluster. An advantage to using the Harbor shared service is that it is managed by Kubernetes, so it provides greater reliability than a stand-alone registry.
```
tanzu package available list harbor.tanzu.vmware.com -A
```

>```
> | Retrieving package versions for harbor.tanzu.vmware.com...
>   NAME                     VERSION               RELEASED-AT                     NAMESPACE
>   harbor.tanzu.vmware.com  2.2.3+vmware.1-tkg.1  2021-07-07 20:00:00 +0200 CEST  default
>```
Create a configuration file named harbor-data-values.yaml: 

```
image_url=$(kubectl -n default get packages harbor.tanzu.vmware.com.2.2.3+vmware.1-tkg.1 -o jsonpath='{.spec.template.spec.fetch[0].imgpkgBundle.image}')
```

```
imgpkg pull -b $image_url -o /tmp/harbor-package-2.2.3
```

>```
> Pulling bundle 'projects.registry.vmware.com/tkg/packages/standard/harbor@sha256:c4fe69a762c3081e2f181dead7d79224b7012db8b74e3ca847798f030f1749ee'
>   Extracting layer 'sha256:daca49ca6a24c1660418741b7c5dbb786cfadca9b25a6161e85ac7713d931fcc' (1/1)
> 
> Locating image lock file images...
> The bundle repo (projects.registry.vmware.com/tkg/packages/standard/harbor) is hosting every image specified in the bundle's Images Lock file (.imgpkg/images.yml)
>
>Succeeded
>```

```
cp /tmp/harbor-package-2.2.3/config/values.yaml harbor-data-values.yaml
```


Set the mandatory passwords and secrets in the harbor-data-values.yaml file by doing one of the following:

```
bash /tmp/harbor-package-2.2.3/config/scripts/generate-passwords.sh harbor-data-values.yaml
```

>```
> Successfully generated random passwords and secrets in harbor-data-values.yaml
>```


Set the hostname setting to the hostname you want to use to access Harbor. For example, harbor.yourdomain.com:
>```yaml
> #! The FQDN for accessing Harbor admin UI and Registry service.
> hostname: harbor.desotech.evan
>```

If you used the generate-passwords.sh script, optionally update the harborAdminPassword with something that is easier to remember:
>```yaml
> #! Required The initial password of Harbor admin.
> harborAdminPassword: ##########
>```

If you are installing Harbor to a Tanzu Kubernetes cluster created by using the Tanzu Kubernetes Grid service, non-empty values are required for the following:

- storageClass: Under persistence.persistentVolumeClaim, for registry, jobservice, database, redis, and trivy set storageClass to a storage profile returned by kubectl get sc:

>```yaml
> persistence:
>   persistentVolumeClaim:
>     registry:
>       #! Use the existing PVC which must be created manually before bound,
>       #! and specify the "subPath" if the PVC is shared with other components
>       existingClaim: ""
>       #! Specify the "storageClass" used to provision the volume. Or the default
>       #! StorageClass will be used(the default).
>       #! Set it to "-" to disable dynamic provisioning
>       storageClass: "tanzu-slow"  #STORAGE-CLASS
>       subPath: ""
>       accessMode: ReadWriteOnce
>       size: 10Gi
>     jobservice:
>       existingClaim: ""
>       storageClass: "tanzu-slow"  #STORAGE-CLASS
>       subPath: ""
>       accessMode: ReadWriteOnce
>       size: 1Gi
>     database:
>       existingClaim: ""
>       storageClass: "tanzu-slow"  #STORAGE-CLASS
>       subPath: ""
>       accessMode: ReadWriteOnce
>       size: 1Gi
>     redis:
>       existingClaim: ""
>       storageClass: "tanzu-slow"  #STORAGE-CLASS
>       subPath: ""
>       accessMode: ReadWriteOnce
>       size: 1Gi
>     trivy:
>       existingClaim: ""
>       storageClass: "tanzu-slow"  #STORAGE-CLASS
>       subPath: ""
>       accessMode: ReadWriteOnce
>       size: 5Gi
>```

pspNames: Set pspNames to PSP values returned by kubectl get psp, for example "vmware-system-restricted,vmware-system-privileged".

>```yaml
> #! The PSP names used by Harbor pods. The names are separated by ','. 'null' means all PSP can be used.
> pspNames: vmware-system-privileged
>```

Remove all comments in the harbor-data-values.yaml file:

```
yq -i eval '... comments=""' harbor-data-values.yaml
```

Install the package:

```
tanzu package install harbor \
--package-name harbor.tanzu.vmware.com \
--version 2.2.3+vmware.1-tkg.1 \
--values-file harbor-data-values.yaml \
--namespace default
```

>```
> \ Installing package 'harbor.tanzu.vmware.com'
> | Getting namespace 'default'
> / Getting package metadata for 'harbor.tanzu.vmware.com'
> | Creating service account 'harbor-default-sa'
> | Creating cluster admin role 'harbor-default-cluster-role'
> | Creating cluster role binding 'harbor-default-cluster-rolebinding'
> | Creating secret 'harbor-default-values'
> - Creating package resource
> | Package install status: Reconciling
>```

### Note
Create a file named overlay-notary-signer-image-fix.yaml with the following contents:
```yaml
#@ load("@ytt:overlay", "overlay")

#@overlay/match by=overlay.and_op(overlay.subset({"kind": "Deployment"}), overlay.subset({"metadata": {"name": "harbor-notary-signer"}}))
---
spec:
  template:
    spec:
      containers:
        #@overlay/match by="name",expects="0+"
        - name: notary-signer
          image: projects.registry.vmware.com/tkg/harbor/notary-signer-photon@sha256:4dfbf3777c26c615acfb466b98033c0406766692e9c32f3bb08873a0295e24d1
```
Issue a command similar to the following to create a secret from the file created in Step 1:

```
kubectl -n default create secret generic harbor-notary-singer-image-overlay -o yaml --dry-run=client --from-file=overlay-notary-signer-image-fix.yaml | kubectl apply -f -
```

Issue a command similar to the following to patch the Harbor package:

```
kubectl -n default annotate packageinstalls harbor ext.packaging.carvel.dev/ytt-paths-from-secret-name.0=harbor-notary-singer-image-overlay
```

### Note 2

```
vi fix-fsgroup-overlay.yaml
```

>```yaml
> #@ load("@ytt:overlay", "overlay")
> 
> #@overlay/match by=overlay.and_op(overlay.subset({"kind": "StatefulSet"}), overlay.subset({"metadata": {"name": "harbor-database"}}))
> ---
> spec:
>   template:
>     spec:
>       initContainers:
>         #@overlay/match by=overlay.index(0)
>         #@overlay/insert before=True
>         - name: "data-ownership-ensurer"
>           securityContext:
>             runAsUser: 0
>           image: projects.registry.vmware.com/tkg/harbor/harbor-db@sha256:26ce0071b528944fd33080f273d0812da479da557eee2727409bd4162719deff
>           imagePullPolicy: IfNotPresent
>           command: ["/bin/sh"]
>           args: ["-c", "chown -R postgres:postgres /var/lib/postgresql/data || true"]
>           volumeMounts:
>             - name: database-data
>               mountPath: /var/lib/postgresql/data
>               subPath:
> 
> #@overlay/match by=overlay.and_op(overlay.subset({"kind": "StatefulSet"}), overlay.subset({"metadata": {"name": "harbor-redis"}}))
> ---
> spec:
>   template:
>     spec:
>       #@overlay/match missing_ok=True
>       initContainers:
>         - name: "redis-ownership-ensurer"
>           securityContext:
>             runAsUser: 0
>           image: projects.registry.vmware.com/tkg/harbor/redis-photon@sha256:5b55e6d2b2da4d8f1eca413c7d79bebfed64fb69db891b80bc4a28f733f1c85e
>           imagePullPolicy: IfNotPresent
>           command: ["/bin/sh"]
>           args: ["-c", "chown -R 999:999 /var/lib/redis || true"]
>           volumeMounts:
>             - name: data
>               mountPath: /var/lib/redis
>               subPath:
>               readOnly: false
> 
> #@overlay/match by=overlay.and_op(overlay.subset({"kind": "StatefulSet"}), overlay.subset({"metadata": {"name": "harbor-trivy"}}))
> ---
> spec:
>   template:
>     spec:
>       #@overlay/match missing_ok=True
>       initContainers:
>         - name: "trivy-ownership-ensurer"
>           securityContext:
>             runAsUser: 0
>           image: projects.registry.vmware.com/tkg/harbor/trivy-adapter-photon@sha256:722bcbe039a3d83bc4cc1d78de1cf533bd38b829d494af288622fa956ca648f8
>           imagePullPolicy: IfNotPresent
>           command: ["/bin/sh"]
>           args: ["-c", "chown -R 10000:10000 /home/scanner/.cache || true"]
>           volumeMounts:
>             - name: data
>               mountPath: /home/scanner/.cache
>               subPath:
>               readOnly: false
> 
> #@overlay/match by=overlay.and_op(overlay.subset({"kind": "Deployment"}), overlay.subset({"metadata": {"name": "harbor-jobservice"}}))
> ---
> spec:
>   template:
>     spec:
>       #@overlay/match missing_ok=True
>       initContainers:
>         - name: "jobservice-ownership-ensurer"
>           securityContext:
>             runAsUser: 0
>           image: projects.registry.vmware.com/tkg/harbor/harbor-jobservice@sha256:1ab9315d6832320413f0ff48b414c26cbcf3beec9a6ccc13a74e07ecffe2a8e0
>           imagePullPolicy: IfNotPresent
>           command: ["/bin/sh"]
>           args: ["-c", "chown -R 10000:10000 /var/log/jobs || true"]
>           volumeMounts:
>             - name: job-logs
>               mountPath: /var/log/jobs
>               subPath:
>               readOnly: false
> 
> #@overlay/match by=overlay.and_op(overlay.subset({"kind": "Deployment"}), overlay.subset({"metadata": {"name": "harbor-registry"}}))
> ---
> spec:
>   template:
>     spec:
>       #@overlay/match missing_ok=True
>       initContainers:
>         - name: "registry-ownership-ensurer"
>           securityContext:
>             runAsUser: 0
>           image: projects.registry.vmware.com/tkg/harbor/harbor-registryctl@sha256:aa7a6547a46b2b0222c7187567e6f85ffbd853611038bf1b0c33a8356d863108
>           imagePullPolicy: IfNotPresent
>           command: ["/bin/sh"]
>           args: ["-c", "chown -R 10000:10000 /storage || true"]
>           volumeMounts:
>             - name: registry-data
>               mountPath: /storage
>               subPath:
>               readOnly: false
>```

```
kubectl -n default create secret generic harbor-database-redis-trivy-jobservice-registry-image-overlay -o yaml --dry-run=client --from-file=fix-fsgroup-overlay.yaml | kubectl apply -f -
```

>```
> secret/harbor-database-redis-trivy-jobservice-registry-image-overlay created
>```

```
kubectl -n default annotate packageinstalls harbor ext.packaging.carvel.dev/ytt-paths-from-secret-name.1=harbor-database-redis-trivy-jobservice-registry-image-overlay
```

>```
> packageinstall.packaging.carvel.dev/harbor annotated
>```

```
tanzu package installed list -A
```

>```
> \ Retrieving installed packages...
>   NAME          PACKAGE-NAME                   PACKAGE-VERSION        STATUS               NAMESPACE
>   cert-manager  cert-manager.tanzu.vmware.com  1.1.0+vmware.1-tkg.2   Reconcile succeeded  default
>   contour       contour.tanzu.vmware.com       1.17.1+vmware.1-tkg.1  Reconcile succeeded  default
>   fluent-bit    fluent-bit.tanzu.vmware.com    1.7.5+vmware.1-tkg.1   Reconcile succeeded  default
>   grafana       grafana.tanzu.vmware.com       7.5.7+vmware.1-tkg.1   Reconcile succeeded  default
>   harbor        harbor.tanzu.vmware.com        2.2.3+vmware.1-tkg.1   Reconcile succeeded  default
>   prometheus    prometheus.tanzu.vmware.com    2.27.0+vmware.1-tkg.1  Reconcile succeeded  default
>```


Obtain the Harbor CA certificate from the harbor-tls secret in the tanzu-system-registry namespace:

```
kubectl -n tanzu-system-registry get secret harbor-tls -o=jsonpath="{.data.ca\.crt}" | base64 -d
```

>```
> -----BEGIN CERTIFICATE-----
> MIIDWzCCAkOgAwIBAgIRAPsMWJyEIKyl1owKvT4glzowDQYJKoZIhvcNAQELBQAw
> LTEXMBUGA1UEChMOUHJvamVjdCBIYXJib3IxEjAQBgNVBAMTCUhhcmJvciBDQTAe
> Fw0yMjAyMjQxNDQ2MDBaFw0zMjAyMjIxNDQ2MDBaMC0xFzAVBgNVBAoTDlByb2pl
> Y3QgSGFyYm9yMRIwEAYDVQQDEwlIYXJib3IgQ0EwggEiMA0GCSqGSIb3DQEBAQUA
> A4IBDwAwggEKAoIBAQCv7p7xgZd+Lc5F8gpspiuhfvd48sfjq1M7lO+32Hah4/kH
> jN5hkysMyUHlsCLtwTuNoQIgzZR2HStJ9OfuldMX2lpBE5HoCmHBgM9IB5mja83U
> Gfy19ifxE4DUnWg0qdvzprcM08FWosdYugzugR5ag5sCO0qDWB1va+lv88g5ThLe
> jw4QjmR+KLdFkWVZpqrioPTAr6x0ZPp6SbB6uoE9YR9EqdTSD+yVXPmR2J2F/kqx
> ErWWfMJGxHgRcuOGMbIwv74m/ACRClNSrXwDZYW+h9eQ9CCBqWuT80MB291kRLD2
> wSxszv+NNTt6fn550GHgTh37DwvIXr6lu8/a0SMNAgMBAAGjdjB0MA4GA1UdDwEB
> /wQEAwICBDAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUHAwIwDwYDVR0TAQH/
> BAUwAwEB/zAdBgNVHQ4EFgQUkUlCSiGREYTFxb2A8NoOJ3ZLSlYwEwYDVR0RBAww
> CoIIaGFyYm9yY2EwDQYJKoZIhvcNAQELBQADggEBAGAa2RvWdpxG1wAFIzqSPDOe
> /fwTC38LffLsjH0qu+eC+kKiRCo0kHRbRH6ti7q9n7zMBEggDrpBcfx6NnYJdAO3
> /jXelPXZtdeHJtdQ6BtxQqTgFGcNW4cP6NhA4hyKONF+k2ORNHn2xq0yz3PdqTjb
> P7qydBAenMihMg0fHbw6JURNG5zU8U1dBINj4NSePmFQI2+tpty2YkxnoTJCJZiL
> h5Vxop8l3QGwhljAEVvfN/dLhnF6DOpX5wVKISiUwNbcywCbzCpwCOfYvVXslgV6
> 0N14Fnb3huFU/kG/Beyje77nomne7SJqScwqk5xL360+olhLMnrVQRF2r5tte7w=
> -----END CERTIFICATE-----
>```

tain the address of the Envoy service load balancer.

```
kubectl get svc -n tanzu-system-ingress
```


If you deployed Harbor on a shared services cluster that is running on vSphere, you must add an IP to hostname mapping in /etc/hosts or add corresponding A records in your DNS server

Users can now connect to the Harbor UI by navigating to https://harbor.yourdomain.com in a Web browser and log in as user admin with the harborAdminPassword that you configured in harbor-data-values.yaml.


If Harbor uses a self-signed certificate, download the Harbor CA certificate from https://harbor.*yourdomain.com*/api/v2.0/systeminfo/getcert



---
Evangelista Tragni

Desotech srl